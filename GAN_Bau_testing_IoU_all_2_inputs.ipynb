{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CzJ2t3J0d45"
      },
      "outputs": [],
      "source": [
        "pip install git+https://github.com/billybraith17/billiespnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RvNwM0Ecg4ac"
      },
      "outputs": [],
      "source": [
        "pip install torch espnet espnet_model_zoo parallel_wavegan==0.5.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxIZkC-7keAd",
        "outputId": "cece2b9c-0138-41d4-ab25-3b3125f0f0d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ninja\n",
            "  Downloading ninja-1.10.2.3-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (108 kB)\n",
            "Installing collected packages: ninja\n",
            "Successfully installed ninja-1.10.2.3\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "!(stat -t /usr/local/lib/*/dist-packages/google/colab > /dev/null 2>&1) && exit\n",
        "pip install ninja 2>> install.log\n",
        "git clone https://github.com/SIDN-IAP/global-model-repr.git tutorial_code 2>> install.log\n",
        "# git clone https://github.com/billybraith17/global-model-repr.git tutorial_code 2>> install.log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZzQNB5nikhAC"
      },
      "outputs": [],
      "source": [
        "try: # set up path\n",
        "    import google.colab, sys, torch\n",
        "    sys.path.append('/content/tutorial_code')\n",
        "    if not torch.cuda.is_available():\n",
        "        print(\"Change runtime type to include a GPU.\")\n",
        "except:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhOKz_lMOAmD"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4V4F8d3OOK6g",
        "outputId": "7a178d7d-ba10-4717-b26d-6721c2154fbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running pytorch 1.12.1+cu113 using cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Running pytorch', torch.__version__, 'using', device.type)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7EfUeMRoOO1N"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "import torch.hub\n",
        "\n",
        "# using the global-model-repr package\n",
        "from netdissect import nethook, proggan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twGwJzyeEZHZ",
        "outputId": "b1cf53ec-c2d3-4fec-e251-985b504c5204"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 54.8 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46CeL7EIlN7C",
        "outputId": "54ae1fde-3d46-46de-e81e-9eb4c3e94fc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/cmudict.zip.\n",
            "https://zenodo.org/record/4394602/files/tts_train_xvector_conformer_fastspeech2_transformer_teacher_raw_phn_tacotron_g2p_en_no_space_train.loss.ave.zip?download=1: 100%|██████████| 270M/270M [00:21<00:00, 13.0MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1oVOC4Vf0DYLdDp4r7GChfgj7Xh5xd0ex\n",
            "To: /root/.cache/parallel_wavegan/vctk_hifigan.v1.tar.gz\n",
            "100%|██████████| 916M/916M [00:03<00:00, 276MB/s]\n",
            "WARNING:root:Fallback to conformer_pos_enc_layer_type = 'legacy_rel_pos' due to the compatibility. If you want to use the new one, please use conformer_pos_enc_layer_type = 'latest'.\n",
            "WARNING:root:Fallback to conformer_self_attn_layer_type = 'legacy_rel_selfattn' due to the compatibility. If you want to use the new one, please use conformer_pos_enc_layer_type = 'latest'.\n"
          ]
        }
      ],
      "source": [
        "# def from_pretrained2(\n",
        "#     model_tag: Optional[str] = None,\n",
        "#     vocoder_tag: Optional[str] = None,\n",
        "#     **kwargs: Optional[Any],\n",
        "# ):\n",
        "\"\"\"Build Text2Speech instance from the pretrained model.\n",
        "Args:\n",
        "    model_tag (Optional[str]): Model tag of the pretrained models.\n",
        "        Currently, the tags of espnet_model_zoo are supported.\n",
        "    vocoder_tag (Optional[str]): Vocoder tag of the pretrained vocoders.\n",
        "        Currently, the tags of parallel_wavegan are supported, which should\n",
        "        start with the prefix \"parallel_wavegan/\".\n",
        "Returns:\n",
        "    Text2Speech: Text2Speech instance.\n",
        "\"\"\"\n",
        "\n",
        "from espnet2.bin.tts_inference import Text2Speech\n",
        "\n",
        "model_tag = 'kan-bayashi/vctk_xvector_conformer_fastspeech2' #@param [\"kan-bayashi/vctk_gst_tacotron2\", \"kan-bayashi/vctk_gst_transformer\", \"kan-bayashi/vctk_xvector_tacotron2\", \"kan-bayashi/vctk_xvector_transformer\", \"kan-bayashi/vctk_xvector_conformer_fastspeech2\", \"kan-bayashi/vctk_gst+xvector_tacotron2\", \"kan-bayashi/vctk_gst+xvector_transformer\", \"kan-bayashi/vctk_gst+xvector_conformer_fastspeech2\", \"kan-bayashi/vctk_multi_spk_vits\", \"kan-bayashi/vctk_full_band_multi_spk_vits\", \"kan-bayashi/libritts_xvector_transformer\", \"kan-bayashi/libritts_xvector_conformer_fastspeech2\", \"kan-bayashi/libritts_gst+xvector_transformer\", \"kan-bayashi/libritts_gst+xvector_conformer_fastspeech2\", \"kan-bayashi/libritts_xvector_vits\"] {type:\"string\"}\n",
        "vocoder_tag = \"parallel_wavegan/vctk_hifigan.v1\"  #@param [\"none\", \"parallel_wavegan/vctk_parallel_wavegan.v1.long\", \"parallel_wavegan/vctk_multi_band_melgan.v2\", \"parallel_wavegan/vctk_style_melgan.v1\", \"parallel_wavegan/vctk_hifigan.v1\", \"parallel_wavegan/libritts_parallel_wavegan.v1.long\", \"parallel_wavegan/libritts_multi_band_melgan.v2\", \"parallel_wavegan/libritts_hifigan.v1\", \"parallel_wavegan/libritts_style_melgan.v1\"] {type:\"string\"}\n",
        "kwargs = {\n",
        "    # Only for Tacotron 2 & Transformer\n",
        "    \"threshold\" : 0.5,\n",
        "    # Only for Tacotron 2\n",
        "    \"minlenratio\" : 0.0,\n",
        "    \"maxlenratio\" : 10.0,\n",
        "    \"use_att_constraint\" : False,\n",
        "    \"backward_window\" : 1,\n",
        "    \"forward_window\" : 3,\n",
        "    # Only for FastSpeech & FastSpeech2 & VITS\n",
        "    \"speed_control_alpha\" : 1.0,\n",
        "    # Only for VITS\n",
        "    \"noise_scale\" : 0.333,\n",
        "    \"noise_scale_dur\" : 0.333,\n",
        "}\n",
        "\n",
        "# print(kwargs[\"noise_scale\"])\n",
        "# model_tag: Optional[str] = None\n",
        "# vocoder_tag: Optional[str] = None\n",
        "# **kwargs: Optional[Any]\n",
        "\n",
        "\n",
        "if model_tag is not None:\n",
        "    try:\n",
        "        from espnet_model_zoo.downloader import ModelDownloader\n",
        "\n",
        "    except ImportError:\n",
        "        logging.error(\n",
        "            \"`espnet_model_zoo` is not installed. \"\n",
        "            \"Please install via `pip install -U espnet_model_zoo`.\"\n",
        "        )\n",
        "        raise\n",
        "    d = ModelDownloader()\n",
        "    kwargs.update(**d.download_and_unpack(model_tag))\n",
        "\n",
        "if vocoder_tag is not None:\n",
        "    if vocoder_tag.startswith(\"parallel_wavegan/\"):\n",
        "        try:\n",
        "            from parallel_wavegan.utils import download_pretrained_model\n",
        "\n",
        "        except ImportError:\n",
        "            logging.error(\n",
        "                \"`parallel_wavegan` is not installed. \"\n",
        "                \"Please install via `pip install -U parallel_wavegan`.\"\n",
        "            )\n",
        "            raise\n",
        "\n",
        "        from parallel_wavegan import __version__\n",
        "        from packaging.version import parse as V\n",
        "        from pathlib import Path\n",
        "\n",
        "        # NOTE(kan-bayashi): Filelock download is supported from 0.5.2\n",
        "        assert V(__version__) > V(\"0.5.1\"), (\n",
        "            \"Please install the latest parallel_wavegan \"\n",
        "            \"via `pip install -U parallel_wavegan`.\"\n",
        "        )\n",
        "        vocoder_tag = vocoder_tag.replace(\"parallel_wavegan/\", \"\")\n",
        "        vocoder_file = download_pretrained_model(vocoder_tag)\n",
        "        vocoder_config = Path(vocoder_file).parent / \"config.yml\"\n",
        "        kwargs.update(vocoder_config=vocoder_config, vocoder_file=vocoder_file)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"{vocoder_tag} is unsupported format.\")\n",
        "\n",
        "# __init__\n",
        "text2speech = Text2Speech(**kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NsnfVGDxxeN7"
      },
      "outputs": [],
      "source": [
        "# Wrap the vocoder\n",
        "from netdissect import nethook\n",
        "# layer = 'vocoder.blocks.11.convs2.0.1'    #@param [\"vocoder.last_conv_layers.0\", \"vocoder.last_conv_layers.1\", \"vocoder.last_conv_layers.2\", \"vocoder.last_conv_layers.3\", \"vocoder.conv_layers.6.conv\", \"vocoder.conv_layers.6.conv1x1_aux\", \"vocoder.conv_layers.6.conv1x1_out\", \"vocoder.conv_layers.6.conv1x1_skip\", \"vocoder.first_conv\", \"vocoder.upsample_net.conv_in\", \"vocoder.upsample_net.upsample.up_layers.6\", \"vocoder.upsample_net.upsample.up_layers.7\", \"vocoder.melgan.0\", \"vocoder.melgan.19.stack.4\", \"vocoder.melgan.23\", \"vocoder.pqmf.pad_fn\", \"vocoder.blocks.7.upsample\", \"vocoder.output_conv.1\", \"vocoder.blocks.11.convs2.0.1\", \"vocoder.output_conv.2\"] {type:\"string\"}\n",
        "\n",
        "# layer = 'vocoder.blocks.0.convs1.0.1'\n",
        "\n",
        "# Don't re-wrap it, if it's already wrapped (e.g., if you press enter twice)\n",
        "if not isinstance(text2speech.vocoder, nethook.InstrumentedModel):\n",
        "    text2speech.vocoder = nethook.InstrumentedModel(text2speech.vocoder)\n",
        "\n",
        "\n",
        "# convolutional layers\n",
        "# HiFiGAN_layers = ['vocoder.input_conv', 'vocoder.blocks.0.convs1.0.1', 'vocoder.blocks.0.convs1.1.1', 'vocoder.blocks.0.convs1.2.1', 'vocoder.blocks.0.convs2.0.1', 'vocoder.blocks.0.convs2.1.1', 'vocoder.blocks.0.convs2.2.1', 'vocoder.blocks.1.convs1.0.1', 'vocoder.blocks.1.convs1.1.1', 'vocoder.blocks.1.convs1.2.1', 'vocoder.blocks.1.convs2.0.1', 'vocoder.blocks.1.convs2.1.1', 'vocoder.blocks.1.convs2.2.1', 'vocoder.blocks.2.convs1.0.1', 'vocoder.blocks.2.convs1.1.1', 'vocoder.blocks.2.convs1.2.1', 'vocoder.blocks.2.convs2.0.1', 'vocoder.blocks.2.convs2.1.1', 'vocoder.blocks.2.convs2.2.1', 'vocoder.blocks.3.convs1.0.1', 'vocoder.blocks.3.convs1.1.1', 'vocoder.blocks.3.convs1.2.1', 'vocoder.blocks.3.convs2.0.1', 'vocoder.blocks.3.convs2.1.1', 'vocoder.blocks.3.convs2.2.1', 'vocoder.blocks.4.convs1.0.1', 'vocoder.blocks.4.convs1.1.1', 'vocoder.blocks.4.convs1.2.1', 'vocoder.blocks.4.convs2.0.1', 'vocoder.blocks.4.convs2.1.1', 'vocoder.blocks.4.convs2.2.1', 'vocoder.blocks.5.convs1.0.1', 'vocoder.blocks.5.convs1.1.1', 'vocoder.blocks.5.convs1.2.1', 'vocoder.blocks.5.convs2.0.1', 'vocoder.blocks.5.convs2.1.1', 'vocoder.blocks.5.convs2.2.1', 'vocoder.blocks.6.convs1.0.1', 'vocoder.blocks.6.convs1.1.1', 'vocoder.blocks.6.convs1.2.1', 'vocoder.blocks.6.convs2.0.1', 'vocoder.blocks.6.convs2.1.1', 'vocoder.blocks.6.convs2.2.1', 'vocoder.blocks.7.convs1.0.1', 'vocoder.blocks.7.convs1.1.1', 'vocoder.blocks.7.convs1.2.1', 'vocoder.blocks.7.convs2.0.1', 'vocoder.blocks.7.convs2.1.1', 'vocoder.blocks.7.convs2.2.1', 'vocoder.blocks.8.convs1.0.1', 'vocoder.blocks.8.convs1.1.1', 'vocoder.blocks.8.convs1.2.1', 'vocoder.blocks.8.convs2.0.1', 'vocoder.blocks.8.convs2.1.1', 'vocoder.blocks.8.convs2.2.1', 'vocoder.blocks.9.convs1.0.1', 'vocoder.blocks.9.convs1.1.1', 'vocoder.blocks.9.convs1.2.1', 'vocoder.blocks.9.convs2.0.1', 'vocoder.blocks.9.convs2.1.1', 'vocoder.blocks.9.convs2.2.1', 'vocoder.blocks.10.convs1.0.1', 'vocoder.blocks.10.convs1.1.1', 'vocoder.blocks.10.convs1.2.1', 'vocoder.blocks.10.convs2.0.1', 'vocoder.blocks.10.convs2.1.1', 'vocoder.blocks.10.convs2.2.1', 'vocoder.blocks.11.convs1.0.1', 'vocoder.blocks.11.convs1.1.1', 'vocoder.blocks.11.convs1.2.1', 'vocoder.blocks.11.convs2.0.1', 'vocoder.blocks.11.convs2.1.1', 'vocoder.blocks.11.convs2.2.1', 'vocoder.output_conv.1']\n",
        "# activation layers for convolutional layers\n",
        "HiFiGAN_layers = ['vocoder.upsamples.0.0', 'vocoder.upsamples.1.0', 'vocoder.upsamples.2.0', 'vocoder.upsamples.3.0', 'vocoder.blocks.0.convs1.0.0', 'vocoder.blocks.0.convs1.1.0', 'vocoder.blocks.0.convs1.2.0', 'vocoder.blocks.0.convs2.0.0', 'vocoder.blocks.0.convs2.1.0', 'vocoder.blocks.0.convs2.2.0', 'vocoder.blocks.1.convs1.0.0', 'vocoder.blocks.1.convs1.1.0', 'vocoder.blocks.1.convs1.2.0', 'vocoder.blocks.1.convs2.0.0', 'vocoder.blocks.1.convs2.1.0', 'vocoder.blocks.1.convs2.2.0', 'vocoder.blocks.2.convs1.0.0', 'vocoder.blocks.2.convs1.1.0', 'vocoder.blocks.2.convs1.2.0', 'vocoder.blocks.2.convs2.0.0', 'vocoder.blocks.2.convs2.1.0', 'vocoder.blocks.2.convs2.2.0', 'vocoder.blocks.3.convs1.0.0', 'vocoder.blocks.3.convs1.1.0', 'vocoder.blocks.3.convs1.2.0', 'vocoder.blocks.3.convs2.0.0', 'vocoder.blocks.3.convs2.1.0', 'vocoder.blocks.3.convs2.2.0', 'vocoder.blocks.4.convs1.0.0', 'vocoder.blocks.4.convs1.1.0', 'vocoder.blocks.4.convs1.2.0', 'vocoder.blocks.4.convs2.0.0', 'vocoder.blocks.4.convs2.1.0', 'vocoder.blocks.4.convs2.2.0', 'vocoder.blocks.5.convs1.0.0', 'vocoder.blocks.5.convs1.1.0', 'vocoder.blocks.5.convs1.2.0', 'vocoder.blocks.5.convs2.0.0', 'vocoder.blocks.5.convs2.1.0', 'vocoder.blocks.5.convs2.2.0', 'vocoder.blocks.6.convs1.0.0', 'vocoder.blocks.6.convs1.1.0', 'vocoder.blocks.6.convs1.2.0', 'vocoder.blocks.6.convs2.0.0', 'vocoder.blocks.6.convs2.1.0', 'vocoder.blocks.6.convs2.2.0', 'vocoder.blocks.7.convs1.0.0', 'vocoder.blocks.7.convs1.1.0', 'vocoder.blocks.7.convs1.2.0', 'vocoder.blocks.7.convs2.0.0', 'vocoder.blocks.7.convs2.1.0', 'vocoder.blocks.7.convs2.2.0', 'vocoder.blocks.8.convs1.0.0', 'vocoder.blocks.8.convs1.1.0', 'vocoder.blocks.8.convs1.2.0', 'vocoder.blocks.8.convs2.0.0', 'vocoder.blocks.8.convs2.1.0', 'vocoder.blocks.8.convs2.2.0', 'vocoder.blocks.9.convs1.0.0', 'vocoder.blocks.9.convs1.1.0', 'vocoder.blocks.9.convs1.2.0', 'vocoder.blocks.9.convs2.0.0', 'vocoder.blocks.9.convs2.1.0', 'vocoder.blocks.9.convs2.2.0', 'vocoder.blocks.10.convs1.0.0', 'vocoder.blocks.10.convs1.1.0', 'vocoder.blocks.10.convs1.2.0', 'vocoder.blocks.10.convs2.0.0', 'vocoder.blocks.10.convs2.1.0', 'vocoder.blocks.10.convs2.2.0', 'vocoder.blocks.11.convs1.0.0', 'vocoder.blocks.11.convs1.1.0', 'vocoder.blocks.11.convs1.2.0', 'vocoder.blocks.11.convs2.0.0', 'vocoder.blocks.11.convs2.1.0', 'vocoder.blocks.11.convs2.2.0', 'vocoder.output_conv.0', 'vocoder.output_conv.2']\n",
        "\n",
        "parallel_wavegan_layers = [\"vocoder.first_conv\", \"vocoder.conv_layers.0.conv\", \"vocoder.conv_layers.0.conv1x1_aux\", \"vocoder.conv_layers.0.conv1x1_out\", \"vocoder.conv_layers.0.conv1x1_skip\", \"vocoder.conv_layers.1.conv\", \"vocoder.conv_layers.1.conv1x1_aux\", \"vocoder.conv_layers.1.conv1x1_out\", \"vocoder.conv_layers.1.conv1x1_skip\", \"vocoder.conv_layers.2.conv\", \"vocoder.conv_layers.2.conv1x1_aux\", \"vocoder.conv_layers.2.conv1x1_out\", \"vocoder.conv_layers.2.conv1x1_skip\", \"vocoder.conv_layers.3.conv\", \"vocoder.conv_layers.3.conv1x1_aux\", \"vocoder.conv_layers.3.conv1x1_out\", \"vocoder.conv_layers.3.conv1x1_skip\", \"vocoder.conv_layers.4.conv\", \"vocoder.conv_layers.4.conv1x1_aux\", \"vocoder.conv_layers.4.conv1x1_out\", \"vocoder.conv_layers.4.conv1x1_skip\", \"vocoder.conv_layers.5.conv\", \"vocoder.conv_layers.5.conv1x1_aux\", \"vocoder.conv_layers.5.conv1x1_out\", \"vocoder.conv_layers.5.conv1x1_skip\", \"vocoder.conv_layers.6.conv\", \"vocoder.conv_layers.6.conv1x1_aux\", \"vocoder.conv_layers.6.conv1x1_out\", \"vocoder.conv_layers.6.conv1x1_skip\", \"vocoder.conv_layers.7.conv\", \"vocoder.conv_layers.7.conv1x1_aux\", \"vocoder.conv_layers.7.conv1x1_out\", \"vocoder.conv_layers.7.conv1x1_skip\", \"vocoder.conv_layers.8.conv\", \"vocoder.conv_layers.8.conv1x1_aux\", \"vocoder.conv_layers.8.conv1x1_out\", \"vocoder.conv_layers.8.conv1x1_skip\", \"vocoder.conv_layers.9.conv\", \"vocoder.conv_layers.9.conv1x1_aux\", \"vocoder.conv_layers.9.conv1x1_out\", \"vocoder.conv_layers.9.conv1x1_skip\", \"vocoder.conv_layers.10.conv\", \"vocoder.conv_layers.10.conv1x1_aux\", \"vocoder.conv_layers.10.conv1x1_out\", \"vocoder.conv_layers.10.conv1x1_skip\", \"vocoder.conv_layers.11.conv\", \"vocoder.conv_layers.11.conv1x1_aux\", \"vocoder.conv_layers.11.conv1x1_out\", \"vocoder.conv_layers.11.conv1x1_skip\", \"vocoder.conv_layers.12.conv\", \"vocoder.conv_layers.12.conv1x1_aux\", \"vocoder.conv_layers.12.conv1x1_out\", \"vocoder.conv_layers.12.conv1x1_skip\", \"vocoder.conv_layers.13.conv\", \"vocoder.conv_layers.13.conv1x1_aux\", \"vocoder.conv_layers.13.conv1x1_out\", \"vocoder.conv_layers.13.conv1x1_skip\", \"vocoder.conv_layers.14.conv\", \"vocoder.conv_layers.14.conv1x1_aux\", \"vocoder.conv_layers.14.conv1x1_out\", \"vocoder.conv_layers.14.conv1x1_skip\", \"vocoder.conv_layers.15.conv\", \"vocoder.conv_layers.15.conv1x1_aux\", \"vocoder.conv_layers.15.conv1x1_out\", \"vocoder.conv_layers.15.conv1x1_skip\", \"vocoder.conv_layers.16.conv\", \"vocoder.conv_layers.16.conv1x1_aux\", \"vocoder.conv_layers.16.conv1x1_out\", \"vocoder.conv_layers.16.conv1x1_skip\", \"vocoder.conv_layers.17.conv\", \"vocoder.conv_layers.17.conv1x1_aux\", \"vocoder.conv_layers.17.conv1x1_out\", \"vocoder.conv_layers.17.conv1x1_skip\", \"vocoder.conv_layers.18.conv\", \"vocoder.conv_layers.18.conv1x1_aux\", \"vocoder.conv_layers.18.conv1x1_out\", \"vocoder.conv_layers.18.conv1x1_skip\", \"vocoder.conv_layers.19.conv\", \"vocoder.conv_layers.19.conv1x1_aux\", \"vocoder.conv_layers.19.conv1x1_out\", \"vocoder.conv_layers.19.conv1x1_skip\", \"vocoder.conv_layers.20.conv\", \"vocoder.conv_layers.20.conv1x1_aux\", \"vocoder.conv_layers.20.conv1x1_out\", \"vocoder.conv_layers.20.conv1x1_skip\", \"vocoder.conv_layers.21.conv\", \"vocoder.conv_layers.21.conv1x1_aux\", \"vocoder.conv_layers.21.conv1x1_out\", \"vocoder.conv_layers.21.conv1x1_skip\", \"vocoder.conv_layers.22.conv\", \"vocoder.conv_layers.22.conv1x1_aux\", \"vocoder.conv_layers.22.conv1x1_out\", \"vocoder.conv_layers.22.conv1x1_skip\", \"vocoder.conv_layers.23.conv\", \"vocoder.conv_layers.23.conv1x1_aux\", \"vocoder.conv_layers.23.conv1x1_out\", \"vocoder.conv_layers.23.conv1x1_skip\", \"vocoder.conv_layers.24.conv\", \"vocoder.conv_layers.24.conv1x1_aux\", \"vocoder.conv_layers.24.conv1x1_out\", \"vocoder.conv_layers.24.conv1x1_skip\", \"vocoder.conv_layers.25.conv\", \"vocoder.conv_layers.25.conv1x1_aux\", \"vocoder.conv_layers.25.conv1x1_out\", \"vocoder.conv_layers.25.conv1x1_skip\", \"vocoder.conv_layers.26.conv\", \"vocoder.conv_layers.26.conv1x1_aux\", \"vocoder.conv_layers.26.conv1x1_out\", \"vocoder.conv_layers.26.conv1x1_skip\", \"vocoder.conv_layers.27.conv\", \"vocoder.conv_layers.27.conv1x1_aux\", \"vocoder.conv_layers.27.conv1x1_out\", \"vocoder.conv_layers.27.conv1x1_skip\", \"vocoder.conv_layers.28.conv\", \"vocoder.conv_layers.28.conv1x1_aux\", \"vocoder.conv_layers.28.conv1x1_out\", \"vocoder.conv_layers.28.conv1x1_skip\", \"vocoder.conv_layers.29.conv\", \"vocoder.conv_layers.29.conv1x1_aux\", \"vocoder.conv_layers.29.conv1x1_out\", \"vocoder.conv_layers.29.conv1x1_skip\",  \"vocoder.last_conv_layers.1\", \"vocoder.last_conv_layers.3\"]\n",
        "\n",
        "\n",
        "for layer in HiFiGAN_layers:\n",
        "  text2speech.vocoder.retain_layer(layer)\n",
        "\n",
        "\n",
        "# \"vocoder.blocks.11.convs2.0.1\", \"vocoder.output_conv.2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_1FJ68l9M7Q"
      },
      "outputs": [],
      "source": [
        "text2speech.vocoder\n",
        "\n",
        "# text2speech.vocoder.retain_layer('vocoder')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-1QugLj-7xZ"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import os\n",
        "import numpy as np\n",
        "import kaldiio\n",
        "\n",
        "# Get model directory path\n",
        "from espnet_model_zoo.downloader import ModelDownloader\n",
        "d = ModelDownloader()\n",
        "model_dir = os.path.dirname(d.download_and_unpack(model_tag)[\"train_config\"])\n",
        "\n",
        "# X-vector selection\n",
        "# We only need the x-vector part at that's what this model uses. The ones with\n",
        "spembs = None\n",
        "if text2speech.use_spembs:\n",
        "  xvector_ark = [p for p in glob.glob(f\"{model_dir}/../../dump/**/spk_xvector.ark\", recursive=True) if \"tr\" in p][0]\n",
        "  xvectors = {k: v for k, v in kaldiio.load_ark(xvector_ark)}\n",
        "  spks = list(xvectors.keys())\n",
        "\n",
        "  # # randomly select speaker\n",
        "  # random_spk_idx = np.random.randint(0, len(spks))\n",
        "  # spk = spks[random_spk_idx]\n",
        "  # spembs = xvectors[spk]\n",
        "  # print(f\"selected spk: {spk}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIK93bOj5Dw7"
      },
      "outputs": [],
      "source": [
        "# acts = text2speech.vocoder.retained_layer('vocoder.blocks.0.convs1.0.0')\n",
        "# acts2 = text2speech.vocoder.retained_layer('vocoder.blocks.0.convs1.1.0')\n",
        "# print(acts.shape, '\\n \\n', acts2.shape)\n",
        "\n",
        "# acts = text2speech.vocoder.retained_layer(layer)\n",
        "# print(acts.shape)\n",
        "# torch.flatten(acts, start_dim=1).shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UvrIRQx1lCX"
      },
      "outputs": [],
      "source": [
        "# Indices of speakers with these properties\n",
        "\n",
        "southern = {0, 3, 4, 6, 7, 14, 30, 31, 41}\n",
        "s_african = {82, 86, 93, 99}\n",
        "indian = {21, 24} # 107\n",
        "scottish = {9, 11, 15, 19, 20, 22, 25, 28, 33, 35, 36, 37, 38, 44, 45, 48, 54, 57, 58}\n",
        "irish = {18, 39, 56, 61, 65, 67, 81, 95, 105}\n",
        "dublin = {18, 61, 65, 95}\n",
        "n_irish = {12, 34, 62, 63, 73, 100}\n",
        "belfast = {12, 34, 62, 63, 73}\n",
        "cali = {64, 68, 69, 85}\n",
        "yorkshire = {40, 43} # 60\n",
        "NE = {42, 50, 55, 59}\n",
        "canada = {71, 72, 76, 80, 83, 84, 97, 104}\n",
        "\n",
        "female = {0, 3,4,5,6, 8, 9, 10, 12, 13, 14, 17, 21, 22, 23, 26, 30, 34, 35, 37, 38, 39, 40, 41, 42, 49, 50, 53, 55, 56, 61, 63, 64, 65, 66, 68, 69, 70, 72, 74, 75, 76, 77, 78, 80, 81,82, 84, 85, 86, 88, 89, 90, 92,93,94,95,96,97, 100, 102,103}\n",
        "male = {1, 2, 7, 11, 15, 16, 18, 19, 20, 24, 25, 27, 28, 29, 31, 32, 33, 36, 43, 44, 45, 46, 47, 48, 51, 52, 54, 57, 58, 59, 60, 62, 67, 71, 73, 79, 83, 87, 91, 98, 99, 101, 104, 105, 106, 107}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HY0PGAfma9uA"
      },
      "outputs": [],
      "source": [
        "# testing to see if we could convert a list to a tensor then use topk to get the values and indices\n",
        "\n",
        "# IoU_list = torch.tensor(IoU_list)\n",
        "# top_IoU = torch.topk(IoU_list, 5)\n",
        "# print(top_IoU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9uyX0Rui6bZ"
      },
      "outputs": [],
      "source": [
        "# # code for if I want to make speech quickly\n",
        "# import soundfile as sf\n",
        "# import time\n",
        "\n",
        "# spk = spks[0]\n",
        "# spembs = xvectors[spk]\n",
        "# wav = text2speech(x, spembs=spembs)[\"wav\"]\n",
        "\n",
        "# sf.write(\"out.wav\", wav.numpy(), 24000, \"PCM_16\")\n",
        "\n",
        "# # Display the audio file below\n",
        "# from IPython.display import display, Audio\n",
        "# display(Audio(wav.view(-1).cpu().numpy(), rate=24000))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # testing code to generalise the proeprties indices when running more than once\n",
        "\n",
        "# chosen_property = male\n",
        "# no_of_runs = 2\n",
        "# # adapt list to the number of runs\n",
        "# chosen_property = np.array(list(chosen_property))\n",
        "# chosen_property = chosen_property*no_of_runs\n",
        "# chosen_property_big = set(chosen_property)\n",
        "# for i in range(no_of_runs-1):\n",
        "#   chosen_property = chosen_property+1\n",
        "#   chosen_property_big = chosen_property_big.union(set(chosen_property))\n",
        "# chosen_property_big"
      ],
      "metadata": {
        "id": "A-wKZYtBzSy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IukBQKqwAB8X",
        "outputId": "785729ae-a455-4b76-81e8-50e05044f4ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/22 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:43: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "/usr/local/lib/python3.7/dist-packages/espnet2/torch_utils/device_funcs.py:29: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:172.)\n",
            "  return to_device(torch.from_numpy(data), device, dtype, non_blocking, copy)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:55: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.upsamples.0.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([1214,   10, 1213, 1308,   12, 1299,    9, 1312,   11, 1001, 1309,  380,\n",
            "        1310, 1215, 1311,  298, 1313, 1003,  381,  360])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.upsamples.1.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([1817,   82, 1699,  252,  197, 1820, 1265, 1830,  983, 1227, 1823,  377,\n",
            "        1825, 1818, 1828, 1695, 1833, 1261,  912,   77])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.upsamples.2.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([5122, 5106, 5118, 5127, 2559, 5126, 5115, 5133, 5089, 5111, 5128, 5108,\n",
            "        5129, 5125, 5130, 5117, 5135, 5113, 5109, 5107])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.upsamples.3.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([ 6738,  5718,  5998,  7874,  4404,  7641,  5822,  9669,  4477,  4609,\n",
            "         7877,  4346,  9109,  7115,  9442,  5843, 17926,  5712,  4401,  4212])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.blocks.0.convs1.0.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([1518, 1434, 1492, 1533,  903, 1528, 1439, 1553,  907, 1429, 1538, 1425,\n",
            "        1543, 1523, 1548, 1488, 1557, 1430, 1426, 1282])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.blocks.0.convs1.1.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([907, 904, 927, 908, 898, 978, 902, 998, 897, 917, 983, 912, 988, 909,\n",
            "        993, 903, 999, 922, 914, 893])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.blocks.0.convs1.2.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([898, 897, 962, 903, 892, 904, 905, 983, 908, 909, 902, 906, 967, 899,\n",
            "        978, 917, 988, 912, 907, 893])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.blocks.0.convs2.0.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([ 59,  62, 162,  47,  52,  68,  54, 192,  63, 117,  57,  64, 167,  67,\n",
            "        187,  69, 197, 147,  73,  53])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.blocks.0.convs2.1.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([833, 795, 830, 839, 740, 838, 825, 850, 750, 755, 840, 720, 843, 835,\n",
            "        845, 828, 855, 790, 735, 705])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.blocks.0.convs2.2.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([1029, 1020, 1028, 1033,  977, 1032, 1024, 1038, 1014, 1017,  260,  967,\n",
            "        1034, 1031, 1036, 1025, 1039, 1019,  972,  370])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.blocks.1.convs1.0.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([1518, 1434, 1492, 1533,  903, 1528, 1439, 1553,  907, 1429, 1538, 1425,\n",
            "        1543, 1523, 1548, 1488, 1557, 1430, 1426, 1282])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.blocks.1.convs1.1.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([ 72,  77, 257,  62,  67, 762,  82, 847, 214, 219, 767, 132, 797,  87,\n",
            "        842,  97, 897, 224, 187, 127])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.blocks.1.convs1.2.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([827,  77, 822, 842, 792, 837, 812, 897, 797, 802, 847, 202, 877, 832,\n",
            "        892, 817, 972, 807, 777,  82])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.blocks.1.convs2.0.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([841, 826, 836, 856,  70, 851,  76, 881, 156, 499, 861, 126, 866, 846,\n",
            "        876, 831, 886, 821, 151,  71])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.blocks.1.convs2.1.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([525, 564, 570, 585, 475, 580, 565, 610, 555, 559, 590, 549, 600, 575,\n",
            "        605, 569, 645, 560, 550,  52])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.blocks.1.convs2.2.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([519, 569, 654, 706, 556, 693, 497, 726, 619, 624, 719, 609, 721, 683,\n",
            "        724, 643, 729, 639, 611, 589])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.blocks.2.convs1.0.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([1518, 1434, 1492, 1533,  903, 1528, 1439, 1553,  907, 1429, 1538, 1425,\n",
            "        1543, 1523, 1548, 1488, 1557, 1430, 1426, 1282])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.blocks.2.convs1.1.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([1523, 1554, 1562, 1518, 1544, 1564, 1557, 1574, 1547, 1549, 1567, 1539,\n",
            "        1569, 1352, 1572, 1559, 1582, 1552, 1542, 1537])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.blocks.2.convs1.2.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([1542, 1549, 1569, 1544, 1557, 1572, 1564, 1677, 1518, 1559, 1574, 1552,\n",
            "        1659, 1547, 1673, 1567, 1678, 1562, 1554, 1513])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.blocks.2.convs2.0.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([34, 36, 59, 49, 24, 56, 44, 69, 41, 54, 31, 39, 64, 26, 66, 46, 71, 61,\n",
            "        51, 29])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.blocks.2.convs2.1.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([167,  81, 165, 100,  86, 174,  91, 222,  87, 104, 176,  83, 200, 169,\n",
            "        201, 106, 224, 105, 103,  89])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.blocks.2.convs2.2.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([529, 535, 569, 530, 544, 574, 564, 639, 554, 555, 531, 539, 575, 534,\n",
            "        629, 565, 644, 559, 540, 536])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.blocks.3.convs1.0.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([ 244,  219, 1274,  367,  192, 1651,  217, 2367,  342,  617, 1671,  467,\n",
            "        2296,  392, 2317,  292, 2678,  642,  492,  442])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.blocks.3.convs1.1.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([2848, 2798, 2828, 5085, 2516, 2878, 2813, 5096, 2723, 2773, 5087,  367,\n",
            "        5091, 2873, 5093, 2823, 5097, 2778, 2342,  444])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.blocks.3.convs1.2.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([1503, 5118, 5125, 5132, 5112, 5128, 1433, 5137, 5113, 5115, 5133, 5098,\n",
            "        5134, 5126, 5135, 5123, 5138, 5116, 5110, 5083])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.blocks.3.convs2.0.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([2687, 1055, 2685, 2712, 2650, 2710, 2675, 2735, 2652, 2655, 2715, 2318,\n",
            "        2720, 2700, 2725, 2680, 2750, 2670, 2335, 2296])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.blocks.3.convs2.1.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([3696, 2846, 3575, 2870,   26, 4795, 3395, 9216, 2557, 3050, 2871, 2896,\n",
            "        4971, 4021, 8404, 3526, 9410, 3150, 2921, 2895])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.blocks.3.convs2.2.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([3736, 3555, 3569, 3825, 3439, 3810,  132, 3943, 3440, 3441, 3905,  177,\n",
            "        3930, 3791, 3936,  133, 3955, 3451, 3423,  162])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.blocks.4.convs1.0.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([ 244,  219, 1274,  367,  192, 1651,  217, 2367,  342,  617, 1671,  467,\n",
            "        2296,  392, 2317,  292, 2678,  642,  492,  442])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.blocks.4.convs1.1.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([2092, 1967, 2067, 2367,  144, 2317, 2038, 4726,  892, 1192, 3161,  269,\n",
            "        4578, 2292, 4603, 2042, 5088, 1567,  344,  142])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.blocks.4.convs1.2.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([ 4335,  3205,  4008,  5141,  2818,  5140,  3218, 11098,  2901,  2918,\n",
            "         5142,   346,  5178,  5139, 11015,  3983, 12045,  3093,  2509,   234])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.blocks.4.convs2.0.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([  73,  781,  847, 1354,  327,  899,   43, 1412,  762,  770, 1372,  274,\n",
            "        1377,  872, 1402,  822, 1424,  772,  299,   47])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.blocks.4.convs2.1.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([10123,  4880, 10119, 10142,  3680, 10140, 10092, 10153,  3690,  4330,\n",
            "        10144,  3597, 10145, 10136, 10148, 10117, 10157,  4833,  3599,  3240])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.blocks.4.convs2.2.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([2259, 4513, 7742, 7759,  664, 7744, 7714, 7782, 2489, 2492, 7768, 2484,\n",
            "        7771, 7743, 7772, 7716, 7783, 2495, 2488, 2471])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.blocks.5.convs1.0.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([ 244,  219, 1274,  367,  192, 1651,  217, 2367,  342,  617, 1671,  467,\n",
            "        2296,  392, 2317,  292, 2678,  642,  492,  442])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.blocks.5.convs1.1.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([5075, 5087, 5095, 5073, 5090, 5086, 2502, 5098, 5091, 5092, 5085, 5088,\n",
            "        5096, 5083, 5097, 5094, 5100, 5093, 5089, 5078])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.blocks.5.convs1.2.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([5087, 2492, 5103, 5105, 5083, 5104, 2493, 5113, 5082, 5100, 5110, 5085,\n",
            "        5111, 5088, 5112, 2494, 5117, 5102, 5098, 2491])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.blocks.5.convs2.0.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([1171, 1436, 1791, 1914, 1725, 1891, 1749, 1978, 1746, 1747, 1935, 1633,\n",
            "        1955, 1435, 1977, 1771, 2020, 1748, 1724, 1517])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.blocks.5.convs2.1.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([2495, 1695, 2494, 2502, 1291, 2501, 2492, 2514, 1331, 1520, 2510,  841,\n",
            "        2512, 2500, 2513, 2493, 2515, 1598, 1113,  635])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.blocks.5.convs2.2.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([ 427, 1049, 1071, 1077,  568, 1075, 1051, 1110,  709,  952, 1088,  517,\n",
            "        1096,  415, 1100, 1052, 1111, 1028,  555,  252])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.blocks.6.convs1.0.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([5936, 1568, 5932,   16, 3969, 6064, 5435, 6502, 4402, 4884, 1184, 3064,\n",
            "        6403, 6058, 6484, 5732, 6668, 5303, 3764, 1760])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.blocks.6.convs1.1.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([13527, 13482, 13485, 13544, 13396, 13543, 13483, 13721, 13402, 13470,\n",
            "        13545, 13382, 13665, 13541, 13667, 13484, 13723, 13471, 13384,  8903])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.blocks.6.convs1.2.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([13485, 13471, 13483, 13793,  9263, 13791, 13473, 17805,  9333,  9419,\n",
            "        13794,  3967, 13916, 13551, 13939, 13482, 17806, 10671,  9126,  3726])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.blocks.6.convs2.0.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([459, 436, 451, 465, 365, 461, 448, 497, 432, 433, 479, 428, 488, 460,\n",
            "        490, 450, 499, 434, 430, 418])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.blocks.6.convs2.1.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([2465, 1130, 2464, 1117, 1169, 2473,  382, 2494, 1171, 1199, 2484, 1154,\n",
            "        2485, 2469, 2492, 2462, 2497, 2459, 1168, 1133])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.blocks.6.convs2.2.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([3060, 2108, 3055, 3907, 1783, 3296, 2295, 4593, 1789, 1935, 4292,    6,\n",
            "        4482, 3295, 4544, 2997, 4594, 1986, 1572, 1523])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.blocks.7.convs1.0.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([5936, 1568, 5932,   16, 3969, 6064, 5435, 6502, 4402, 4884, 1184, 3064,\n",
            "        6403, 6058, 6484, 5732, 6668, 5303, 3764, 1760])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.blocks.7.convs1.1.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([2468, 1324, 1388, 2818, 1210, 2817, 1329, 3812, 1260, 1273, 2983, 1128,\n",
            "        3166, 2532, 3551, 1330, 3818, 1322, 1130, 1014])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.blocks.7.convs1.2.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([1123, 1052, 1077, 1131,  294, 1129, 1070, 1223, 1012, 1013, 1136,  953,\n",
            "        1199, 1128, 1212, 1074, 1249, 1014,  956,  860])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.blocks.7.convs2.0.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([  854,  1148, 16418,   652,  1546, 18201,  4270, 21774,  1548,  1734,\n",
            "        19667,  1251, 19718, 16419, 19719, 16417, 21822,  2034,  1448,  1182])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.blocks.7.convs2.1.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([2259,  516, 2158,  280,  372, 3537,  519, 3911,  427,  448,  285,  337,\n",
            "        3772, 2711, 3775,  624, 3988,  513,  365,  333])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.blocks.7.convs2.2.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([5834, 5381, 5809, 3302, 4872, 6147, 5727, 6556, 5290, 5294, 6155, 4810,\n",
            "        6382, 6143, 6544, 5772, 6594, 5377, 4868,  161])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.blocks.8.convs1.0.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([5936, 1568, 5932,   16, 3969, 6064, 5435, 6502, 4402, 4884, 1184, 3064,\n",
            "        6403, 6058, 6484, 5732, 6668, 5303, 3764, 1760])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.blocks.8.convs1.1.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([ 4410,   842,  4004, 20153,  1769,  4602,  3960, 23242,  3250,  3702,\n",
            "        20170,   651, 21753,  4412, 22280,  3961, 23506,  3959,  1632,  1630])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.blocks.8.convs1.2.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([ 4410, 30671, 30854, 30889, 18929, 30870, 30672, 31122, 30138, 30148,\n",
            "        30938,  8827, 31065, 30861, 31066, 30793, 31146, 30661, 18928,  4880])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.blocks.8.convs2.0.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([5110, 4548, 4864, 7271, 3717, 7270, 4549, 9432, 3949, 4135, 8115, 2262,\n",
            "        8826, 6416, 9082, 4863, 9433, 4136, 2263, 2260])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.blocks.8.convs2.1.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([20729, 10199, 20619, 20803,   934, 20802,   286, 20960, 10006, 10153,\n",
            "        20817,   932, 20949, 20801, 20959, 19424, 20971, 10155,   933,   522])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.blocks.8.convs2.2.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([   3,  475, 2692, 3592, 1945, 2920, 2462, 3951, 2268, 2404, 3692, 1758,\n",
            "        3693, 2795, 3694, 2576, 3952, 2406, 1759, 1756])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.blocks.9.convs1.0.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([56431, 51448, 53384, 27114, 50629, 60841, 52657, 61225, 50758, 50781,\n",
            "        60851, 31969, 60857, 56940, 61222, 53383, 61234, 50782, 50338, 31825])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.blocks.9.convs1.1.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([31739, 31637, 31734, 31757, 31095, 31744, 31705, 31905, 31529, 31538,\n",
            "        31843,  2095, 31878, 31743, 31886, 31706, 31911, 31636, 30966,  1577])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.blocks.9.convs1.2.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([ 1688,  1345,  1675,  1850,  1338,  1690,  1546,  2070,   915,  1340,\n",
            "         1915,  1278,  1930,  1689,  2069,  1555, 20270,  1341,  1291,  1142])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.blocks.9.convs2.0.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([ 3158,  3016,  3029,  3180,  2420,  3160,  3021,  3376,  2442,  2577,\n",
            "         3181,   817,  3202,  3159,  3203,  3028, 29900,  2867,  2393,  1276])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.blocks.9.convs2.1.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([  612, 11308, 12181,   613,  7105, 15193, 11309, 17996,  9481,  9525,\n",
            "        17140,  1664, 17539, 15178, 17728, 11311, 18961,  9710,  6705,  1146])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.blocks.9.convs2.2.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([5564, 5486, 5561, 5798, 5314, 5755, 5495, 5851, 5439, 5448, 5800, 4873,\n",
            "        5831, 5632, 5833, 5500, 5861, 5469, 5236, 4714])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.blocks.10.convs1.0.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([56431, 51448, 53384, 27114, 50629, 60841, 52657, 61225, 50758, 50781,\n",
            "        60851, 31969, 60857, 56940, 61222, 53383, 61234, 50782, 50338, 31825])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.blocks.10.convs1.1.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([  978,   932, 31527, 31531,  2178, 31530, 31419, 31933,  3007, 28046,\n",
            "        31536,  1390, 31537, 31528, 31904, 31519, 31934, 30446,  1648,  1021])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.blocks.10.convs1.2.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([1004, 1658, 1670, 1326, 1542, 1747,  927, 1759, 1572, 1634, 1752, 1532,\n",
            "        1753, 1742, 1758, 1665, 1843, 1643, 1533, 1529])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.blocks.10.convs2.0.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([  607,   600, 31752,   817, 31554, 31754, 31705, 31833, 31595, 31635,\n",
            "        31759, 31381, 31780, 31753, 31816, 31737, 31844, 31636, 31418, 29961])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.blocks.10.convs2.1.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([1498, 1414, 1497, 1108, 1390, 1515, 1423, 1527, 1399, 1400, 1524, 1277,\n",
            "        1525, 1507, 1526, 1482, 1528, 1401, 1285, 1185])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.blocks.10.convs2.2.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([2055, 2043, 2054,  690,  613, 2091, 2052, 2136, 2040, 2041, 2102, 2038,\n",
            "        2134,  671, 2135, 2053, 2145, 2042, 2039, 1867])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.blocks.11.convs1.0.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([56431, 51448, 53384, 27114, 50629, 60841, 52657, 61225, 50758, 50781,\n",
            "        60851, 31969, 60857, 56940, 61222, 53383, 61234, 50782, 50338, 31825])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.blocks.11.convs1.1.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([  400,   394,   399,   520,    72,   453,   395, 61667,    69,   379,\n",
            "        22909,   369, 27802,   401, 29720,   397, 61670,   388,   370,   228])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.blocks.11.convs1.2.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([18, 10,  1, 16, 12,  8,  9, 20, 11,  5, 15, 13,  7, 17,  3,  4, 22,  0,\n",
            "         6, 14])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.blocks.11.convs2.0.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([60057, 41851, 60028, 60135, 41405, 60129, 41853, 60148, 41430, 41622,\n",
            "        60139, 13204, 60144, 60090, 60147, 41855, 60152, 41845, 41404, 13194])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.blocks.11.convs2.1.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([28044, 27254, 28043, 41494, 26724, 38970, 27532, 43521, 26725, 27252,\n",
            "        41495, 13513, 41496, 32675, 41497, 28042, 44276, 27253, 26179, 13216])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.blocks.11.convs2.2.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([ 6422,  5807,  5811, 11735,   292, 11732,   353, 19974,  5803,  5805,\n",
            "        12725,   583, 17120, 10705, 17126,  5808, 20540,  5806,  5800,   291])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.output_conv.0 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806]),\n",
            "indices=tensor([1162, 1126, 1161, 1204, 1019, 1183, 1139, 1288, 1064, 1070, 1221,  670,\n",
            "        1266,  502, 1272, 1160, 1289, 1071,  952,  539])) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocoder.output_conv.2 \n",
            " torch.return_types.topk(\n",
            "values=tensor([0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806,\n",
            "        0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0806, 0.0720,\n",
            "        0.0720, 0.0720]),\n",
            "indices=tensor([27243, 12409, 20149, 28033, 11468, 27245, 12410, 12411, 11838, 11839,\n",
            "        28034,  6791, 28035, 27244, 12408,  6792,  6790,  5817,  5820,  5816])) \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# DON'T RUN\n",
        "# MAKE SURE YOU CHANGE THE FILE NAME FIRST\n",
        "import soundfile as sf\n",
        "import time\n",
        "\n",
        "# the ultimate loop\n",
        "import pandas as pd\n",
        "from netdissect import tally\n",
        "\n",
        "# HERE\n",
        "# set text we want our GAN to synthesise\n",
        "text_list = ['Please call Stella. Ask her too.', 'Bring these things with her from the store.']\n",
        "# x = \"Please call Stella. Ask her too.\"\n",
        "\n",
        "# defining speaker indices\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "no_of_runs = 2\n",
        "zds = torch.linspace(0, 108*no_of_runs-1, 108*no_of_runs)\n",
        "\n",
        "# dictionary to store the results\n",
        "data = {}\n",
        "\n",
        "# HERE\n",
        "# property we want to calculate the IoU for\n",
        "chosen_property = female\n",
        "# adapt list to the number of runs\n",
        "chosen_property = np.array(list(chosen_property))\n",
        "chosen_property = chosen_property*no_of_runs\n",
        "chosen_property_big = set(chosen_property)\n",
        "for i in range(no_of_runs-1):\n",
        "  chosen_property = chosen_property+1\n",
        "  chosen_property_big = chosen_property_big.union(set(chosen_property))\n",
        "\n",
        "\n",
        "\n",
        "# loop over each layer in the network\n",
        "for layer in HiFiGAN_layers:\n",
        "  # To collect stats, define a function that returns 2d [samples, units]\n",
        "  def compute_samples(index):\n",
        "      # print(index)\n",
        "      # the index we will use to choose the speaker and the text we use\n",
        "      spk_idx = index[0] // no_of_runs\n",
        "      text_idx = index[0] % no_of_runs\n",
        "\n",
        "      spk = spks[int(spk_idx)]\n",
        "      spembs = xvectors[spk]\n",
        "      wav = text2speech(text_list[int(text_idx)], spembs=spembs)[\"wav\"]\n",
        "\n",
        "      acts = text2speech.vocoder.retained_layer(layer)\n",
        "      acts = torch.flatten(acts, start_dim=1)\n",
        "      acts_stack = acts\n",
        "\n",
        "      for i in index[1:]:\n",
        "        spk_idx = i // no_of_runs\n",
        "        text_idx = i % no_of_runs\n",
        "\n",
        "        spk = spks[int(spk_idx)]\n",
        "        spembs = xvectors[spk]\n",
        "        wav = text2speech(text_list[int(text_idx)], spembs=spembs)[\"wav\"]\n",
        "        acts = text2speech.vocoder.retained_layer(layer)\n",
        "        acts = torch.flatten(acts, start_dim=1)\n",
        "        # print(acts.shape)\n",
        "\n",
        "        # sf.write(\"out.wav\", wav.numpy(), 24000, \"PCM_16\")\n",
        "\n",
        "        # # Display the audio file below\n",
        "        # from IPython.display import display, Audio\n",
        "        # display(Audio(wav.view(-1).cpu().numpy(), rate=24000))\n",
        "\n",
        "        acts_stack = torch.cat((acts_stack, acts), 0)\n",
        "      return acts_stack\n",
        "\n",
        "\n",
        "# use topk to collect the speaker indices which cause the units to activate most\n",
        "  topk = tally.tally_topk(compute_samples, zds)\n",
        "\n",
        "  #calculate IoUs for this layer\n",
        "  IoU_list = []\n",
        "  result = topk.result()[1]\n",
        "  result = result.tolist()\n",
        "  # len(result)\n",
        "\n",
        "# HERE\n",
        "  for i in range(0, topk.result()[1].shape[0]):\n",
        "    # which quantile we want for the threshold\n",
        "    top = result[i][0:10]\n",
        "    top = set(top)\n",
        "    num = top.intersection(chosen_property_big)\n",
        "    den = top.union(chosen_property_big)\n",
        "    IoU_list.append(len(num) / len(den))\n",
        "\n",
        "# select the 20 largest IoUs in the layer and find their indices\n",
        "  IoU_list = torch.tensor(IoU_list)\n",
        "  top_IoU = torch.topk(IoU_list, 20)\n",
        "  print(layer, '\\n', top_IoU, '\\n')\n",
        "  data[layer] = [IoU_list, top_IoU[0], top_IoU[1]]\n",
        "\n",
        "# HERE\n",
        "# write dictionary to pandas dataframe\n",
        "  frame = pd.DataFrame(data, index = ['All IoUs', 'Top IoUs', 'Indices of top IoUs'])\n",
        "  frame.to_csv('female_100_10x2.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBladgZh_m2J"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}